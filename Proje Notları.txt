Projede kullanÄ±lan model eÄŸitme metotlarÄ±:
1ï¸âƒ£ Denetimli Ã–ÄŸrenme (Supervised Learning): 
real/ â†’ 0
ai/   â†’ 1
Ne istediÄŸimizi biliyoruz
â€œAI mi gerÃ§ek mi?â€ net bir soru
Etiketli veri var

2ï¸âƒ£ Transfer Learning
Modeli sÄ±fÄ±rdan eÄŸitmiyoruz.
Bunun yerine:
ImageNetâ€™te eÄŸitilmiÅŸ bir model alÄ±yoruz
Kendi problemimize uyarlÄ±yoruz
model = resnet18(pretrained=True)
model.fc = nn.Linear(..., 2)
Model zaten ÅŸunlarÄ± biliyor:
Kenar
Doku
Åekil
Renk geÃ§iÅŸleri
Biz ona sadece ÅŸunu Ã¶ÄŸretiyoruz:
â€œBunlar AIâ€™a benziyor, bunlar gerÃ§eÄŸe.â€

3ï¸âƒ£ Mini-Batch Gradient Descent
Model:
Tek tek gÃ¶rsel âŒ
TÃ¼m dataset âŒ
ğŸ‘‰ 32â€™lik gruplar halinde Ã¶ÄŸrenir.
Neden?
Daha stabil Ã¶ÄŸrenme
Daha az bellek
Daha hÄ±zlÄ±

4ï¸âƒ£ Backpropagation (Geri YayÄ±lÄ±m)
Modelin en Ã¶nemli Ã¶ÄŸrenme mekanizmasÄ±.
AdÄ±mlar:
Tahmin yap
Hata hesapla
HatanÄ±n hangi aÄŸÄ±rlÄ±ktan geldiÄŸini bul
O aÄŸÄ±rlÄ±klarÄ± dÃ¼zelt
Kod karÅŸÄ±lÄ±ÄŸÄ±:
loss.backward()

5ï¸âƒ£ Adam Optimizer + CrossEntropy Loss
â€œTahmin ile gerÃ§ek etiket arasÄ±ndaki fark ne kadar?â€
criterion = nn.CrossEntropyLoss()
SÄ±nÄ±flandÄ±rma iÃ§in standart
Softmax + log iÅŸlemlerini iÃ§erir

Optimizer â€” Adam
ğŸ“Œ GÃ¶revi:
â€œBu hatayÄ± nasÄ±l dÃ¼zelteceÄŸiz?â€
optimizer = optim.Adam(model.parameters(), lr=0.0001)
Adam:
Ã–ÄŸrenme hÄ±zÄ±nÄ± otomatik ayarlar
GÃ¼rÃ¼ltÃ¼lÃ¼ datada Ã§ok iyi
BaÅŸlangÄ±Ã§ iÃ§in en gÃ¼venlisi


ğŸ“ŒBU METOTLAR BÄ°RLÄ°KTE NASIL Ã‡ALIÅIR?
KÄ±saca:
GÃ¶rsel
  â†“
ResNet (Transfer Learning)
  â†“
Tahmin (AI / Real)
  â†“
CrossEntropyLoss (Hata)
  â†“
Backpropagation
  â†“
Adam (AÄŸÄ±rlÄ±klarÄ± gÃ¼ncelle)
  â†“
Tekrar



Model = Ã–ÄŸrenci
Dataset = Ders kitabÄ±
Loss = YanlÄ±ÅŸ cevap
Optimizer = Ã‡alÄ±ÅŸma yÃ¶ntemi
Epoch = Deneme sÄ±navÄ±